\sgcomment{We write here about 
\begin{enumerate}
    \item Stackelberg games including history (Refer to 1934 paper by Stackelberg); comparison with Nash equilibrium and state that Stackelberg strategy may produce greater payoff for the leader than in the Nash equilibrium.
    \item application of Stackelberg games
\end{enumerate}}

Stackelberg games were first introduced by German economist Heinrich Freiherr von Stackelberg in his book \textit{Marktform und Gleichgewicht} in 1934 \cite{S34} to better simulate how firms behave in the market. The players of this game comprise of one leader firm and follower firms. The leader starts the game by announcing her strategy and the followers respond by playing the optimal reponse to the leader's strategy.

% Let us demonstrate why Stackelberg Equilibria produce a better payoff than their Nash counterparts with an example. Consider the period of 1894 - 1999 where the motorcycle manufacturers were in a competition to produce the fastest production motorcycle. After over a century of one-upmanship, some regulators and politicians in Europe, fearing an outbreak of illegal racing as riders try to break the 200 mph barrier, called for an import ban against high speed motorcycles. To preempt regulation and avoid negative publicity, the manufacturers voluntarily entered into a \text{Gentlemen's agreement} to limit the speed of their machines to 300 kmph (186 mph), starting with 2000 models. A Nash Equilibrium strategy would suggest that each company must break the agreement to produce a faster motorcycle, thus getting a higher market share. But breaking the agreement would likely result in an import ban regulation which would imply a huge market loss for all manufacturers involved. On the other hand, a Stackelberg Equilibrium strategy would explain why that no company must break the agreement, and thus would ideally yield equal market share for all motorcycle companies involved.\cite{WIKI00}

We demonstrate here with a mean-payoff game example why Stackelberg Equilibrium produces a better payoff than a Nash equilibrium strategy. Consider the example depicted in \cref{fig:nash_vs_stackelberg}. Here, vertices of the leader, Player~0, are depicted with blue circles and the vertices of the follower, Player~1, are depicted with red squares.

\begin{figure}
    \centering
    \begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.8cm,
                        semithick, squarednode/.style={rectangle, draw=blue!60, fill=blue!5, very thick, minimum size=15mm}]
      \tikzstyle{every state}=[fill=white,draw=black,text=black,minimum size=2cm]
    
      \node[squarednode]                                                 (A)              {$v_0$};
      \node[state, draw=red!60, fill=red!5]                              (B) [left of=A]  {$v_1$};
      \node[state, draw=red!60, fill=red!5]                              (C) [right of=A] {$v_3$};
      \node[draw=none, fill=none, minimum size=0cm, node distance = 2cm] (D) [above of=A] {$start$};
      \node[state, draw=red!60, fill=red!5]                              (E) [left of=B]  {$v_2$};
      \path (A) edge              node {(1,2)} (B)
                edge              node {(0,1)} (C)
            (B) edge [loop above] node {(1,2)} (B)
                edge              node {(2,0)} (E)
            (C) edge [loop above] node {(0,1)} (C)
            (E) edge [loop above] node {(2,0)} (E)
            (D) edge [left] node {} (A);
    \end{tikzpicture}
    \caption{An example in which Stackelberg equilibrium for Player $0$ gives better payoff than Nash equilibrium.}
    \label{fig:nash_vs_stackelberg}
\end{figure}

In this paper, we study the notion of Stackelberg Equilibria on two-player non-zero sum infinite duration mean-payoff games played on graph arenas. We note $\Sigma_0$ the set of strategies of Player~0, also called the leader, and $\Sigma_1$ the set of strategies of Player~1, also called the follower. Additionally, the game comes with two (usually $\mathbb{R}$-valued) mean-payoff functions, $\mathbf{MP}_0$ and $\mathbf{MP}_1$, that determine the payoff each player receives: if $\sigma_0 \in \Sigma_0$ and $\sigma_1 \in \Sigma_1$ are chosen then Player~0 receives the payoff $\mathbf{MP}_0 (\mathsf{Out}(\sigma_0$, $\sigma_1))$ while Player~1 receives the payoff $\mathbf{MP}_1(\mathsf{Out}(\sigma_0$, $\sigma_1))$, where $\mathsf{Out}(\sigma_0$, $\sigma_1)$ refers to the play which is the outcome of Player~0 and Player~1 playing their respective strategies. Both players aim at maximizing their respective payoffs. As mentioned before, in a Stackelberg game, players play sequentially as follows.
\begin{inparaenum}[(i)]
\item Player~0, the leader, announces her choice of strategy $\sigma_0 \in \Sigma_0$. 
\item Player~1, the follower, announces his choice of strategy $\sigma_1 \in \Sigma_1$ in response to the leader's strategy. 
\item Both players receive their respective payoffs: $\mathbf{MP}_0 (\mathsf{Out}(\sigma_0$, $\sigma_1))$ and $\mathbf{MP}_1 (\mathsf{Out}(\sigma_0$, $\sigma_1))$.
\end{inparaenum}
Due to the sequential nature of the game, Player~1 knows the strategy $\sigma_0$, and so to act rationally (s)he should choose a strategy $\sigma_1$ that maximizes the payoff $\mathbf{MP}_1 (\mathsf{Out}(\sigma_0$, $\sigma_1))$. If such a strategy $\sigma_1$ exists, it is called a best-response to the strategy $\sigma_0 \in \Sigma_0$.
In this paper, we assume that Player~1 is not completely rational, and hence Player~1 would choose an $\epsilon$-optimal best-response to the strategy $\sigma_0 \in \Sigma_0$. In \cite{FGR20}, we establish that there always exists an $\epsilon$-optimal best response for Player~1 for every strategy $\sigma_0 \in \Sigma_0$ of Player~0.
In turn, if the leader assumes an almost-rational response of the follower to her strategy, this should guide the leader when choosing $\sigma_0 \in \Sigma_0$.
Indeed, the leader should choose a strategy $\sigma_0 \in \Sigma_0$ such that the value $\mathbf{MP}_0 (\mathsf{Out}(\sigma_0$, $\sigma_1))$ is as large as possible when $\sigma_1$ is an $\epsilon$-optimal best-response of the follower.

Two different scenarios can be considered in this setting: either the best-response $\sigma_1 \in \Sigma_1$ is imposed by the leader (or equivalently chosen cooperatively by the two players), or the best-response is chosen adversarially by Player~1. But, the adversarial case is more interesting because it allows us to model the situation in which the leader can only choose $\sigma_0 \in \Sigma_0$ and must be prepared to face any almost rational response of Player~1, i.e. if Player~1 has several possible $\epsilon$-best responses then $\sigma_0$ should be designed to face all of them. In this paper, our main contribution is to investigate the second route.

\textbf{Related Work} In \cite{FKL10}, we study the notion of rational synthesis in the co-operative setting, whereas in \cite{KPV16}, we study the notion of rational synthesis in adversarial setting. In \cite{CFGR16}, we study the complexity of rational synthesis for various omega-regular objectives in both the adversarial and co-operative setting. In \cite{GS14}, we study multi-player mean-payoff Stackelberg games in the co-operative setting (here the leader assigns strategies to herself and her followers). 

% In \cite{GS18}, we study Bi-Matrix Stackelberg games in both co-operative and adversarial setting. In \cite{GSTDP16}, we study the concept of Incentive Stackelberg mean-payoff games, where in addition to assigning strategies to all players, the leader can also transfer parts of her payoff to other players to incentivise to follow the assigned strategy. In \cite{CHJ06}, we study the concept of Secure Equilibrium in two-player non-zero sum games where the players are adversarial.  This is very similar to Adversarial Stackelberg Equilibrium but in the case of Adversarial Secure Equilibrium, there exists no leader player in the game. 

\track{In \cite{GS18}, the authors study both Stackelberg (leader) equilibrium and incentive equilibrium over bimatrix games. It is not difficult to see that the leader can improve upon the payoff by providing an incentive to her follower which is actually a share of her own payoff. They define the leader strategy profile as one in which the follower cannot improve his payoff by deviating, and hence every Nash equlibrium is a leader strategy profile. A leader equilibrium is a leader strategy profile that gives the highest payoff to the leader. Every leader strategy profile is an incentive strategy profile with incentive value $0$. Since in this work, the authors consider mixed strategies, the existence of Nash equilibrium in bimatrix games \cite{Nash50,LH64} implies the existence of leader and incentive equilibria.}

\track{In \cite{CHJ06}, the authors study secure Nash equilibrium, where the objectives are considered in a lexicographic order. The two players try to first maximise their own payoffs, and then minimize the payoff of the other player. A secure Nash equilibrium is a Nash equilibrium.}

We illustrate these equilibria stated above with the help of a bimatrix game.
\sgcomment{Add the bimatrix example from the slides}
Consider the bimatrix game shown in Table \ref{tab:bimatrix}. Considering pure strategy profiles, the only pure strategy Nash equilibrium (and thus a secure Nash equilibrium) is the profile (I, I), while the profile (II, II) is a leader equilibrium.

In \cite{GSTDP16}, we study the concept of Incentive Stackelberg mean-payoff games, where in addition to assigning strategies to all players, the leader can also transfer parts of her payoff to other players to incentivise to follow the assigned strategy. 

In \cite{PFKSV14}, we study Secure Equilibrium in multi-player games.  In \cite{GS15}, we study the effects of limited memory on both Nash and Stackelberg Equilibria in Multi-player discounted sum games. In \cite{FGR20}, we study Stackelberg Mean-Payoff Games in adversarial setting and  Stackelberg Discount Sum Games in both adversarial and co-operative setting.

\track{Adversarial Stackelberg games can be relevant in several contexts. Consider two belligerent countries, say $A$ and $B$ that are engaged in increasing their nuclear arsenal. There may be a country, let's say $A$ that is more powerful than the other. While both the countries are under mutual threat, however, it is likely that the less powerful country gets more seriously affected in the case of a war. So it is in the interest of the more powerful country to attack the less powerful one. On the other hand, any attack from the more powerful country $A$ on the other one may lead the country $B$ to stop exporting to $A$ some goods that are of high value to $A$. The resultant state models the situation of a Nash equilibrium . If the more powerful country, on the other hand, pledges to commit to a nuclear non-proliferation, and asks country $B$ to do the same, it is likely that the less powerful country will also follow the same, and both the countries save expenses on increasing their nuclear arsenal. However, still since the less powerful country $B$ does not have a cordial relation with country $A$, it will try to act in a way that harms  country $A$ whenever possible modelling the case of an Adversarial Stackelberg game.}

\track{In \cite{GS18}, the authors consider a class of Arms Race game as an example for incentive equilibrium.}


\sgcomment{{\bf Related Work} We should look at the following papers:
\begin{enumerate}
    \item Dana Fisman, Orna Kupferman, and Yoad Lustig. Rational synthesis. (Even these papers possibly talk about the cooperative setting, and not the adversarial setting.)
    \item Orna Kupferman, Giuseppe Perelli, and Moshe Y. Vardi. Synthesis with rational environments.
    \item Rodica Condurache, Emmanuel Filiot, Raffaella Gentilini, and Jean-François Raskin. The complexity of rational synthesis.
    \item Emmanuel Filiot, Raffaella Gentilini, and Jean-François Raskin. Rational synthesis under imperfect information. \mbcomment{(If it is imperfect information, it is not Stackelberg as P0 announces strategy, right?)}
    \item Anshul Gupta and Sven Schewe. Quantitative verification in rational environments.
    \item Anshul Gupta, Sven Schewe. Buying Optimal Payoffs in Bi-Matrix Games
    \item Anshul Gupta, Sven Schewe, Ashutosh Trivedi, Maram Sai Krishna Deepak, and Bharath Kumar Padarthi. Incentive stackelberg mean-payoff games.
    \item \mbcomment{We should also look at the literature for stochastic Stackelberg games}
    \item Krishnendu Chatterjee, Thomas A. Henzinger, Marcin Jurdzinski. Games with Secure Equilibria
    \item Julie De Pril, János Flesch, Jeroen Kuipers, Gijs Schoenmakers, Koos Vrieze. Existence of Secure Equilibrium in Multi-player Games with Perfect Information
    \item Anshul Gupta, Sven Schewe, and Dominik Wojtczak. Making the best of limited memory in multi-player discounted sum games
    \item Emmanuel Filiot, Raffaella Gentilini, and Jean-François Raskin. The Adversarial Stackelberg Value in Quantitative Games
\end{enumerate}}

\textbf{\bf Our contribution} In contrast with the work in \cite{FGR20}, we assume here that Player~1 is not completely rational, i.e., he is willing to get an $\epsilon$ amount lesser than the best payoff possible. We first show that, in general, infinite memory is required for Player 0 to achieve the $\mathbf{ASV}^{\epsilon}$. This is true for achieving $\mathbf{ASV}$ as well, as show in \cite{FGR20}. We also show that $\epsilon$-optimal best-responses of Player 1 may require infinite memory. Similar to the results in \cite{FGR20}, we introduce a notion of $\epsilon$-witness for proving that $\mathbf{ASV}^{\epsilon}$ is greater than some threshold and show that a finite memory strategy, which can be found in $\mathsf{NP}$-time, is sufficient to achieve this threshold. Furthermore, we describe an $\mathsf{EXP}$-time algorithm to compute the $\mathbf{ASV}^{\epsilon}$, which is in a improvement (time-wise) on the algorithm to compute $\mathbf{ASV}$ in \cite{FGR20}. Additionally, we provide an improvement on the $\mathsf{NP}$-time algorithm for threshold problem in \cite{FGR20} and show that a finite memory strategy is sufficient to achieve $\mathbf{ASV} > c$. In contrast to the results in \cite{FGR20}, we show that $\mathbf{ASV}^{\epsilon}$ is always achievable.

\textbf{Structure of the Paper} In \cref{sec:prelim}, we introduce the necessary preliminaries for our definitions and developments. In \cref{sec:examples}, we examine the memory requirements of both players for playing their strategies. In \cref{sec:ThresholdProblem}, we consider the threshold problem, i.e. checking if $\mathbf{ASV}^{\epsilon} > c$. In \cref{sec:ComputeASV}, we present an algorithm to compute the $\mathbf{ASV}^{\epsilon}$. In \cref{sec:FMASV}, we present a memory improvement for a previous result established in \cite{FGR20}. In \cref{sec:Achievability}, we show that the $\mathbf{ASV}^{\epsilon}$ is always achievable.