% \sgcomment{Add an intro for this section: What this section is about.}
In this section, we consider finite memory strategies of Player~$0$ for both $\mathbf{ASV}$, as well as $\mathbf{ASV}^\epsilon$. We show that for every vertex $v$, if there exists a strategy $\sigma_0$ for Player~0 such that $\mathbf{ASV}(\sigma_0)(v) > c$), then there exists a finite memory strategy $\sigma_0^{FM}$ such that $\mathbf{ASV}(\sigma_0^{FM})(v) > c$. This further implies that $\mathbf{ASV}(v) = \sup\limits_{\sigma_0 \in \Sigma_0^{\mathsf{FM}}} \mathbf{ASV}(\sigma_0)(v)$ (\textbf{\cref{CorASVEqASVFinNonEps}}).

\textbf{Modified Game:} We begin with the construction of a modified game $\mathcal{G'}$ from the given game $\mathcal{G}$ by multiplying the first dimension on each edge of the game by $-1$. It is easy to see that, for all vertices $v \in \mathcal{G}$, we have that $v \nvDash \ll 1 \gg \underline{\mathbf{MP}}_0 \leqslant c \land \underline{\mathbf{MP}}_1 \geqslant d$ if and only if for the corresponding vertex in the game $\mathcal{G'}$, we have $v \nvDash \ll 1 \gg \overline{\mathbf{MP}}_0 \geqslant -c \land \underline{\mathbf{MP}}_1 \geqslant d$.\\ 
\noindent We modify the game $\mathcal{G'}$ further by adding $c$ to the first dimension and subtracting $d$ from the second dimension for all the edges in the game. Note that the edges and vertices of both games $\mathcal{G}$ and $\mathcal{G'}$ are the same, the only difference between the two games being their edge weights.

\begin{proposition}
\label{PropGameStrEqNewGameStrNonEps}
For every vertex $v \in V$ in the game $\mathcal{G'}$, Player 0 (Player 1) can ensure that $v \nvDash \ll 1 \gg \overline{\mathbf{MP}}_0 \geqslant 0 \land \underline{\mathbf{MP}}_1 \geqslant 0$ ($v \models \ll 1 \gg \overline{\mathbf{MP}}_0 \geqslant 0 \land \underline{\mathbf{MP}}_1 \geqslant 0$) with a strategy $\sigma_0$ ($\sigma_1$) if and only if Player 0 (Player 1) can ensure that $v \nvDash \ll 1 \gg \underline{\mathbf{MP}}_0 \leqslant c \land \underline{\mathbf{MP}}_1 \geqslant d$ ($v \models \ll 1 \gg \underline{\mathbf{MP}}_0 \leqslant c \land \underline{\mathbf{MP}}_1 \geqslant d$) with the same strategy $\sigma_0$ ($\sigma_1$) from vertex $v$ in the game $\mathcal{G}$.
\end{proposition}

We then state the following result for Multi-dimensional two-player games with $\mathbf{MeanPayoffInfSup}$ objective \cite{VCDHRR15}.

\begin{theorem}
\label{ThmMemlessStrForP2NonEps}
\textbf{\emph{(Theorem 8 in \cite{VCDHRR15})}} For multi-dimensional two-player mean-payoff games with objective \\
$\mathbf{MeanPayoffInfSup}(I,J) = \{\pi \in \mathbf{Plays}(\mathcal{G}) \mid \forall i \in I : \underline{\mathbf{MP}}(\pi)_i \geqslant 0 \text{ and } \forall j \in J : \overline{\mathbf{MP}}(\pi)_j \geqslant 0\}$ for Player 1, the following assertions hold:
\begin{enumerate}
    \item Winning strategies for Player 1 require infinite-memory in general, and memoryless winning strategies exist for Player 0. \footnote{Player 0 is called Player 2 in \cite{VCDHRR15}}
    \item The problem of deciding whether a given vertex is winning for Player 1 is coNP-complete.
\end{enumerate}
\end{theorem}

Now, we are ready to show the existence of finite memory strategy for $\mathbf{ASV}(v) > c$ with the following theorem:

\begin{lemma}
\label{LemFinMemWitnessASVNonEps}
Given a mean-payoff game $\mathcal{G}$, a vertex $v \in V$, a rational value $c \in \mathbb{Q}$, if $\mathbf{ASV}(v) > c$, then there exists a finite memory strategy, $\sigma_0$, for Player 0 such that $\mathbf{ASV}(\sigma_0)(v) > c$.
\end{lemma}

\begin{proof}
Given that $\mathbf{ASV}(v) > c$, from \textbf{Lemma 10} in \cite{FGR20}, we know that there exists three non-cyclic finite paths $\pi_1, \pi_2, \pi_3$ and two simple cycles $l_1, l_2$, such that a play $\pi = \pi_1\rho_1\rho_2\rho_3\dots$ , where $\rho_i = l_1^{[\alpha i]}.\pi_2.l_2^{[\beta i]}.\pi_3$. We call this path $\pi$ a witness for $\mathbf{ASV}(v) > c$. Note that with $\pi$, we can construct an infinite memory strategy for Player 0 so as to minimize the effect of $\pi_2$ and $\pi_3$ on the mean-payoff measure. Let $\underline{\mathbf{MP}}_0(\pi) = c'' > c$ and $\underline{\mathbf{MP}}_1(\pi) = d$.

\noindent The mean-payoffs of the play $\pi$ are: $\underline{\mathbf{MP}}_0(\pi) = \alpha \cdot w_0(l_1) + \cdot \beta w_0(l_2)$ and $\underline{\mathbf{MP}}_1(\pi) = \alpha \cdot w_1(l_1) + \beta \cdot w_1(l_2)$. To obtain a finite memory strategy, we can modify the play $\pi$ as described below:
\begin{caseof}
    \case{$w_0(l_1) > w_0(l_2)$ and $w_1(l_1) < w_1(l_2)$}
    Here, one simple cycle, $l_1$, increases Player 0's mean-payoff while the other simple cycle, $l_2$, increases Player 1's mean-payoff. We can build a witness $\pi' = \pi_1.(l_1^{[\alpha]k}.\pi_2.l_2^{[\beta+\tau]k}.\pi_3)^{\omega}$ for some very large $k \in \mathbb{N}$ and for some very small $\tau > 0$ such that $\underline{\mathbf{MP}}_0(\pi') > c$ and $\underline{\mathbf{MP}}_1(\pi') = d$. \footnote{For more details, we refer the reader to the Appendix.}
    \case{$w_0(l_1) < w_0(l_2)$ and $w_1(l_1) > w_1(l_2)$}
    This is analogous to \textbf{case 1} and proceeds as mentioned above.
    \case{$w_0(l_1) < w_0(l_2)$ and $w_1(l_1) < w_1(l_2)$}
    One cycle, $l_2$, increases both Player 0 and Player 1's mean-payoff, while the other, $l_1$, decreases it. In this case, we can just omit one of the cycles - whichever cycle gives a larger mean-payoff, to get a finite memory strategy. Thus, $\pi' = \pi_1.l_2^{\omega}$ and we get $\underline{\mathbf{MP}}_0(\pi') > c$ , $\underline{\mathbf{MP}}_1(\pi') \geqslant d$.
    \case{$w_0(l_1) > w_0(l_2)$ and $w_1(l_1) > w_1(l_2)$}
    This is analogous to \textbf{case 3} and proceeds as mentioned above.
\end{caseof}
In each of these cases, we can show that $\pi'$ is a witness for $\mathbf{ASV}(v) > c$: $\underline{\mathbf{MP}}_0(\pi') > c$ and $\underline{\mathbf{MP}}_1(\pi') \geqslant d$. Since we know that in \textbf{Lemma 10} of \cite{FGR20} that $\pi$ does not cross a $(c,d)$-bad vertex, the modified play $\pi'$ does not either, as the vertices of the play $\pi'$ are a subset of the vertices of the play $\pi$. 

Thus, using $\pi'$, we can build a finite memory strategy $\sigma_0$ for Player 0:
\begin{enumerate}
    \item Player 0 follows $\pi'$ if Player 1 does not deviate from $\pi'$. The finite memory strategy stems from the large value of $k$ as required in the four cases mentioned above.
    \item \label{strategy_memoryless_non_epsilon} For each vertex $v \in \pi'$, Player 0 employs a memoryless strategy that establishes $v \nvDash \ll 1 \gg \underline{\mathbf{MP}}_0 \leqslant c \land \underline{\mathbf{MP}}_1 \geqslant d$.
\end{enumerate}
Thus, we see that $\mathbf{ASV}(\sigma_0)(v) > c$.
The existence of the memoryless strategies mentioned in Point \ref{strategy_memoryless_non_epsilon} above is established below. \\ \\
\noindent The goal here is to obtain a memoryless strategy for Player 0 to achieve her objective, i.e., from all vertices $v \in \pi'$, to ensure that $v \nvDash \ll 1 \gg \underline{\mathbf{MP}}_0 \leqslant c \land \underline{\mathbf{MP}}_1 \geqslant d$. \\
\noindent We begin by constructing a game $\mathcal{G'}$ from the given game $\mathcal{G}$ Thus, we get a modified objective for Player 0 in $\mathcal{G'}$, i.e., Player 0 wins if for all vertices $v \in \pi'$, we have that $v \nvDash \ll 1 \gg \overline{\mathbf{MP}}_0 \geqslant 0 \land \underline{\mathbf{MP}}_1 \geqslant 0$.

Now the existence of a memoryless winning strategy for Player 0 follows from \textbf{\cref{ThmMemlessStrForP2NonEps}}. We formulate $\mathcal{G'}$ as a multi-weighted two-player game structure specified in \textbf{\cref{ThmMemlessStrForP2NonEps}} with $I = \{1\}$ and $J = \{0\}$. By \textbf{\cref{ThmMemlessStrForP2NonEps}}, we know that Player 0 has a memoryless winning strategy, $\sigma_0$,  for all vertices $v \in \pi'$ in the game $\mathcal{G'}$ (since she wins for the same vertices in the game $\mathcal{G}$). Thus, by \textbf{\cref{PropGameStrEqNewGameStrNonEps}}, the finite memory strategy $\sigma_0$ is also winning for Player 0 in $\mathcal{G}$ for all vertices $v \in \pi'$, that is, $\mathbf{ASV}(\sigma_0)(v) > c$.
\end{proof}

Now, we establish that, in a mean-payoff game $\mathcal{G}$, the $\mathbf{ASV}$ value of a vertex $v$ in the game $\mathcal{G}$ does not change even if Player 0 is restricted to using only finite memory strategies. Formally, we state that:

\begin{corollary}
\label{CorASVEqASVFinNonEps}
In a mean-payoff game $\mathcal{G}$, if $\mathbf{ASV}(v) = c$, then for every $c' < c$, there exists a finite memory strategy $\sigma_0^{FM}$ for Player 0 which achieves $\mathbf{ASV}(\sigma_0^{FM})(v) > c'$. This implies that $\sup\limits_{\sigma_0 \in \Sigma_0^{\mathsf{FM}}} \mathbf{ASV}(\sigma_0)(v) = \mathbf{ASV}(v) = c$. Thus, $\mathbf{ASV}_{\mathsf{FM}}(v) = \mathbf{ASV}(v)$
\end{corollary}

Additionally, we note when we fix the $\epsilon$ value for $\mathbf{ASV}$, we are able to obtain a finite memory strategy that achieves $\mathbf{ASV}^{\epsilon}(v) > c$. This follows from the result established in \textbf{\cref{ThmNpForASV}}. Thus, we can establish \textbf{\Cref{CorASVEqASVFinNonEps}} for the case of $\mathbf{ASV}^{\epsilon}(v)$. Formally, we state that:

\begin{corollary}
\label{CorASVEqASVFin}
In a mean-payoff game $\mathcal{G}$, if $\mathbf{ASV^{\epsilon}}(v) = c$, then for every $c' < c$, there exists a finite memory strategy $\sigma_0^{FM}$ for Player 0 which achieves $\mathbf{ASV^{\epsilon}}(\sigma_0^{FM})(v) > c'$. This implies that $\sup\limits_{\sigma_0 \in \Sigma_0^{\mathsf{FM}}} \mathbf{ASV^{\epsilon}}(\sigma_0)(v) = \mathbf{ASV^{\epsilon}}(v) = c$. Thus, $\mathbf{ASV}^{\epsilon}_{\mathsf{FM}}(v) = \mathbf{ASV}^{\epsilon}(v)$
\end{corollary}