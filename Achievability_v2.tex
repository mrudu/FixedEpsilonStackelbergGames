% \sgcomment{Add an intro for this section: What this section is about.}
While it has been shown in \cite{FGR20} that \textbf{$\mathbf{ASV}$} may not be achievable, in this section, we show that $\mathbf{ASV}^{\epsilon}$ is indeed achievable (\ref{ConjAchiev}).
The entire section is devoted towards proving this theorem.

\textbf{Witness for $\mathbf{ASV}^{\epsilon}(\sigma_0^i)$}
Given a mean-payoff game $\mathcal{G}$ and an $\epsilon > 0$, we say that a play $\pi_i$ is a witness for $\mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i$ for the strategy $\sigma_0^i$ of Player 0 if the strategy $\sigma_0^i$ is defined as follows:
\begin{itemize}
    \item Player 0 follows $\pi_i$ if Player 1 does not deviate from $\pi_i$.
    \item If Player 1 deviates $\pi_i$, then for each vertex $v \in \pi_i$, Player 0 employs a memoryless strategy that establishes $v \nvDash \ll 1 \gg \underline{\mathbf{MP}}_0 \leqslant c_i \land \underline{\mathbf{MP}}_1 > d_i - \epsilon$, where $d_i = \underline{\mathbf{MP}}_1(\pi_i)$.
\end{itemize}

\begin{conjecture}
\label{ConjAchiev}
For all mean-payoff games $\mathcal{G}$, for all vertices $v$ in $\mathcal{G}$ and for all $\epsilon > 0$ we have that $\mathbf{ASV}^{\epsilon}(v)$ is achievable.
\end{conjecture}
\begin{proof}
Let $\mathbf{ASV}^{\epsilon}(v) = c$.

From \textbf{\cref{CorASVEqASVFin}}, we know that $\mathbf{ASV}^{\epsilon}(v) = \mathbf{ASV}^{\epsilon}_{\mathsf{FIN}}(v)$, i.e., the Adversarial Stackleberg Value for fixed $\epsilon$ from vertex $v$ of a game $\mathcal{G}$ remains the same even if we restrict Player 0 to only using finite memory strategies.

% Without loss of generality, we can disregard the mean-payoff games where $\mathbf{ASV}^{\epsilon}(v)$ can be achieved by a finite memory strategy of Player 0. Hence, we consider the mean-payoff games where Player 0 requires at least infinite memory to achieve $\mathbf{ASV}^{\epsilon}(v)$.

We consider below the interesting case, where $\mathbf{ASV}^{\epsilon}(v)$ cannot be achieved by a finite memory strategy.
We show that for such cases it can indeed be achieved by an infinite memory strategy.
 
For every $c' < c$, from \textbf{\cref{LemFinMemWitnessASV}}, we can see that there exists a finite memory strategy $\sigma_0$ such that $\mathbf{ASV}^{\epsilon}(\sigma_0)(v) > c'$.


Now, consider a sequence of increasing real numbers $c_1 < c_2 < c_3 < \dotsc < c$ for which there exist finite memory strategies $\sigma_0^{1}, \sigma_0^{2}, \sigma_0^{3}, \dotsc$ such that for each $c_i$, we have that $ \mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i$.
We note from \textbf{\cref{LemFinMemWitnessASV}} that there exists a witness $\pi_i$ for $ \mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i$ for each $\sigma_0^i$ where $\pi_i= \pi_{1i}(l^{\alpha \cdot k_i}_{1i} \cdot \pi_{2i} \cdot l^{\beta \cdot k_i}_{2i} \cdot \pi_{3i})^{\omega}$, i.e, the strategy $\sigma_0^i$ is described as follows:
\begin{itemize}
    \item Player 0 follows $\pi_i$ if Player 1 does not deviate from $\pi_i$.
    \item If Player 1 deviates $\pi_i$, then for each vertex $v \in \pi_i$, Player 0 employs a memoryless strategy that establishes $v \nvDash \ll 1 \gg \underline{\mathbf{MP}}_0 \leqslant c_i \land \underline{\mathbf{MP}}_1 > d_i - \epsilon$, where $d_i = \underline{\mathbf{MP}}_1(\pi_i)$.
\end{itemize}

\begin{proposition}
\label{PropConvergenceStrategies}
There exists a sequence of increasing real numbers, $c_1 < c_2 < c_3 < \dotsc < c$, and finite memory strategies $\sigma_0^1, \sigma_0^2, \sigma_0^3, \dotsc$ of Player 0 such that for each $c_i$, we have $\mathbf{ASV}^{\epsilon} (\sigma_0^i)(v) > c_i$, and  $\forall i, j \in \mathbb{N}$, for the plays $\pi_i= \pi_{1i}(l^{\alpha_i \cdot k_i}_{1i} \cdot \pi_{2i} \cdot l^{\beta_i \cdot k_i}_{2i} \cdot \pi_{3i})^{\omega}$ and the play $\pi_j= \pi_{1j}(l^{\alpha_j \cdot k_j}_{1j} \cdot \pi_{2j} \cdot l^{\beta_j \cdot k_j}_{2j} \cdot \pi_{3j})^{\omega}$ which are witnesses for $ \mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i$ and $ \mathbf{ASV}^{\epsilon}(\sigma_0^j)(v) > c_j$ respectively, we have that:
$\alpha_i = \alpha_j = \alpha, \beta_i = \beta_j = \beta, \pi_{1i} = \pi_{1j} = \pi_1, \pi_{2i} = \pi_{2j} = \pi_2, \pi_{3i} = \pi_{3j} = \pi_3, l_{1i} = l_{1j} = l_1$ and $l_{2i} = l_{2j} = l_2$ where $\pi_1, \pi_2$ and $\pi_3$ are some finite plays and $l_1, l_2$ are simple cycles in the game $\mathcal{G}$.
\end{proposition}
\begin{proof}
For every $\delta > 0$, there exists $i \in \mathbb{N}$ such that $c - \delta < c_i < c$. Now, we consider the play $\pi_i = \pi_{1i}(l^{\alpha \cdot k_i}_{1i} \cdot \pi_{2i} \cdot l^{\beta \cdot k_i}_{2i} \cdot \pi_{3i})^{\omega}$ which is a witness for $ \mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i$ for the strategy $\sigma_0^i$ such that $\mathbf{ASV}^{\epsilon} (\sigma_0^i)(v) > c_i$. Let $\underline{\mathbf{MP}}_0(\pi_i)= c' > c_i$. 

Without loss of generality, assume that $\underline{\mathbf{MP}}_0(\pi_i)$ increases proportionally with $i$ as $\alpha \cdot k_i$ and $\beta \cdot k_i$ increase. We can make this assumption because we disregard the cases where $\mathbf{ASV}^{\epsilon}(v)$ is achievable with some finite memory strategy of Player 0, i.e. we only consider the case where $\mathbf{ASV}^{\epsilon}(v)$ is not achievable by a finite memory strategy of Player 0.

Now we can construct a series of $(\pi_i)$ such that the  mean-payoff of Player 0 in $\pi_i$ increases with $i$, i.e., $\underline{\mathbf{MP}}_0(\pi_{1i}(l^{\alpha \cdot k_i}_{1i} \cdot \pi_{2i} \cdot l^{\beta \cdot k_i}_{2i} \cdot \pi_{3i})^{\omega}) = c' > c_i, \underline{\mathbf{MP}}_0(\pi_{1i}(l^{\alpha \cdot k_{i+1}}_{1i} \cdot \pi_{2i} \cdot l^{\beta \cdot k_{i+1}}_{2i} \cdot \pi_{3i})^{\omega}) = c'' > c', \underline{\mathbf{MP}}_0(\pi_{1i}(l^{\alpha \cdot k_{i+2}}_{1i} \cdot \pi_{2i} \cdot l^{\beta \cdot k_{i+2}}_{2i} \cdot \pi_{3i})^{\omega}) = c''' > c''$, and so on. Note that in the sequence of $(\pi_i)$, the finite plays $\pi_{1i}, \pi_{2i}, \pi_{3i}$ and the simple cycles $l_{1i}, l_{2i}$ remain constant.

Thus, at the limit when $\delta \to 0$, there must exist a converging sequence of $(\pi_i)$  such that $\lim \limits_{i \to \infty} \underline{\mathbf{MP}}_0(\pi_{1i}(l^{\alpha \cdot i}_{1i} \cdot \pi_{2i} \cdot l^{\beta \cdot i}_{2i} \cdot \pi_{3i})^{\omega}) = c$. For every $i \in \mathbb{N}$, let $\underline{\mathbf{MP}}_0(\pi_i) > c_i$ for some $c_i$. We note here that, for every $i \in \mathbb{N}$, there exists a finite memory strategy $\sigma_0^i$ which has a witness $\pi_i$ for $ \mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i$. We also note that the sequence $(c_i)$ is strictly increasing with $\lim \limits_{i \to \infty} c_i = c$. Thus the only difference in the strategy $\sigma_0^i$ as $i$ changes are the values of $\alpha \cdot k_i$ and $\beta \cdot k_i$, i.e, we increase the value of $\alpha \cdot k_i$ and $\beta \cdot k_i$ proportionally such that the effects of $\pi_2$ and $\pi_3$ on the mean-payoffs are minimised. By construction, we get that $\forall i, j \in \mathbb{N}$, for the plays $\pi_i= \pi_{1i}(l^{\alpha_i \cdot k_i}_{1i} \cdot \pi_{2i} \cdot l^{\beta_i \cdot k_i}_{2i} \cdot \pi_{3i})^{\omega}$ and the play $\pi_j= \pi_{1j}(l^{\alpha_j \cdot k_j}_{1j} \cdot \pi_{2j} \cdot l^{\beta_j \cdot k_j}_{2j} \cdot \pi_{3j})^{\omega}$ which are witnesses for $ \mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i$ and $ \mathbf{ASV}^{\epsilon}(\sigma_0^j)(v) > c_j$, we have that:
$\alpha_i = \alpha_j = \alpha, \beta_i = \beta_j = \beta, \pi_{1i} = \pi_{1j}, \pi_{2i} = \pi_{2j}, \pi_{3i} = \pi_{3j}, l_{1i} = l_{1j} = l_1$ and $l_{2i} = l_{2j}$.
\end{proof}

\begin{remark} 
\label{RemarkStratEqual} 
We note that for the sequence of strategies $(\sigma_0^i)_{i \in \mathbb{N}}$, we have that the memoryless strategies which Player 0 uses when Player 1 deviates from $\pi_i$ are the same for all $i \in \mathbb{N}$.
\end{remark}

For the sequence of strategies $(\sigma_0^i)_{i \in \mathbb{N}}$, we note that the mean-payoff of Player 1 for the play $\pi_i$ changes monotonically with $i$. This is a result of proportional increase of $\alpha \cdot k_i$ and $\beta \cdot k_i$ with $i$. Formally, for the sequence of plays $(\pi_i)_{i \in \mathbb{N}}$ which are witnesses $(\mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i)_{i \in \mathbb{N}}$ by the strategies $(\sigma_0^i)_{i \in \mathbb{N}}$, we let $\underline{\mathbf{MP}}_1(\pi_i) = d_i$. Thus, we claim:

\begin{proposition}
\label{PropMonotonicDi}
The sequence $(d_i)_{i \in \mathbb{N}}$ is monotonic. 
\end{proposition}

We have that $\lim \limits_{i \to \infty} \mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) = c$. Let $\pi_{i \to \infty} = \pi_* = \pi_1 \rho_1 \rho_2 \rho_3 \dotsc$ where $\rho_i = (l^{\alpha}_{1})^i \cdot \pi_{2} \cdot (l^{\beta}_{2})^i \cdot \pi_{3}$. We have that $\underline{\mathbf{MP}}_0(\pi_*) = c$, and we let $\underline{\mathbf{MP}}_1(\pi_*) = d$. Note that at the limit, Player 0 uses an infinite memory strategy. Let $\sigma_0^*$ be this infinite memory strategy of Player 0.

Now, we prove that any play that could be enforced by Player 1 in response to Player 0's strategy $\sigma_0^*$ could be enforced in response to any strategy $\sigma_0^i$ of Player 0, for all $i \in \mathbb{N}$. Formally, we state that:

\begin{proposition}
\label{PropContradictionPlayEnforcable}
If Player 1 deviates from the play $\pi_*$ by playing strategy $\sigma_1'$ to get an outcome $\pi' = \mathbf{Out}(\sigma_0^*, \sigma_1')$, then Player 1 could have played the same strategy in response to $\sigma_0^i$ to get the same outcome $\pi'$, for all $i \in \mathbb{N}$. 
\end{proposition}
\begin{proof}
From \textbf{\cref{PropConvergenceStrategies}}, we know that the sequence of $(\pi_i)$s, for all $i \in \mathbb{N}$, differ only in the $\alpha \cdot k_i$ and $\beta \cdot k_i$ values.
For every $i \in \mathbb{N}$, we describe the strategy $\sigma_0^i$ which ensures that $\mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i$. Now, we recall that if Player 1 deviates from $\pi_i$, then for each vertex $v$ appearing in the path $\pi_i$, Player 0 employs a memoryless strategy that establishes $v \nvDash \ll 1 \gg \underline{\mathbf{MP}}_0 \leqslant c_i \land \underline{\mathbf{MP}}_0 > d_i - \epsilon$.

From \textbf{\cref{RemarkStratEqual}}, we know that the memoryless strategy employed by Player 0 is the same for all $\sigma_0^i$, for every $i \in \mathbb{N}$. Thus, at the limit $i \to \infty$, if Player 1 deviates from $\pi_*$, Player 0 employs the same memoryless strategy which it would have apped if Player 1 had deviated from any play $\pi_i$, for all $i \in \mathbb{N}$.
\end{proof}

Thus, from the above propositions, we now construct a sequence of increasing numbers $c_1 < c_2 < c_3 < \dotsc < c$ such that:
\begin{itemize}
    \item For every $i \in \mathbb{N}$, we construct a strategy $\sigma_0^i$ which ensures $\mathbf{ASV}^{\epsilon}(\sigma_0^i)(v) > c_i$.
    \item The strategy $\sigma_0^i$ follows a path $\pi_i = \pi_{1}(l^{\alpha \cdot k_i}_{1} \cdot \pi_{2} \cdot l^{\beta \cdot k_i}_{2} \cdot \pi_{3})^{\omega}$ where $\pi_1, \pi_2$ and $\pi_3$ are some finite plays and $l_1, l_2$ are simple cycles in the game $\mathcal{G}$.
    \item We let $\underline{\mathbf{MP}}_0(\pi_i) = d_i$. The sequence $(d_i)_{i\in \mathbb{N}}$ is monotonic.
    \item The memoryless strategies of Player 0 in $\sigma_0^i$ which Player 0 employs when Player 1 deviates is the same from every vertex $v$ in $\pi_i$ across all $i \in \mathbb{N}$.
\end{itemize}

At the limit, we assume that Player 0 uses the strategy $\sigma_0^*$. If the $\mathbf{ASV}^{\epsilon}$ is not achievable, then there exists a strategy of Player 1 to enforce some play $\pi'$ such that $\underline{\mathbf{MP}}_0(\pi') = c' < c$ and $\underline{\mathbf{MP}}_1(\pi') = d' > d - \epsilon$. Now, we use the monotonicity of the sequence ($d_i$)$_{i \in \mathbb{N}}$ to show a contradiction. 

Since the sequence ($d_i$)$_{i \in \mathbb{N}}$ is monotonic, there can be two cases.
\begin{enumerate}
    \item The sequence ($d_i$)$_{i \in \mathbb{N}}$ is monotonically non-decreasing.
    \item The sequence ($d_i$)$_{i \in \mathbb{N}}$ is monotonically decreasing.
\end{enumerate}

We start with the first case; the sequence ($d_i$)$_{i \in \mathbb{N}}$ is non-decreasing.

Assume for contradiction that $\mathbf{ASV}^{\epsilon}(v)$ is not achievable, i.e. Player 1 deviates from $\sigma_0^*$ to enforce the path $\pi'$. 

Since ($d_i$)$_{i \in \mathbb{N}}$ is non-decreasing, for all $j \in \mathbb{N}$ we have that $d' > d_j - \epsilon$. We know that the sequence ($c_i$)$_{i \in \mathbb{N}}$ is increasing. Thus, there exists a $j \in \mathbb{N}$ such that $c' \leqslant c_j$. Note that if $\pi'$ is a result of the deviation of Player 1 following some path $\pi_{j'}$ for some index $j'$, we consider the $j$ which is also greater than $j'$.

Consider the strategy $\sigma_0^j$ of Player 0 which follows the play $\pi_j$. We know that $\underline{\mathbf{MP}}_0(\pi_j) > c_j$ and $\underline{\mathbf{MP}}_1(\pi_j) = d_j$. We also know from \textbf{\cref{LemFinMemWitnessASV}} that the play $\pi_j$ does not cross a $(c_j, d_j)^{\epsilon}$-bad vertex. 

*****************************************

From \textbf{\cref{PropContradictionPlayEnforcable}}, we have that Player 1 can enforce the play $\pi'$ in response to the strategy $\sigma_0^j$ of Player 0. As $\underline{\mathbf{MP}}_0(\pi') < c_j$ and $ \underline{\mathbf{MP}}_1(\pi') > d_j-\epsilon$ and $\pi'(0) = \pi_j(0) = v$, it follows that the play $\pi_j$ crosses a $(c_j, d_j)^{\epsilon}$-bad vertex $v$. Thus, the existence of $\pi'$ is a contradiction, which implies that $\mathbf{ASV}^{\epsilon}$ is achievable.

Now, we consider the case where the sequence ($d_i$)$_{i \in \mathbb{N}}$ is monotonically decreasing.

Again, we assume for contradiction that $\mathbf{ASV}^{\epsilon}(v)$ is not achievable.  i.e. Player 1 deviates from $\sigma_0^*$ to enforce the path $\pi'$.

Since the sequence ($d_i$)$_{i \in \mathbb{N}}$ is monotonically decreasing, we know that there must exist $i \in \mathbb{N}$ such that $d' > d_i - \epsilon$. We also note that $\forall j \geqslant i: d' > d_j - \epsilon$.

We know that since ($c_i$)$_{i \in \mathbb{N}}$ is a strictly increasing sequence, there must exist $j \geqslant i$ such that $c_j > c'$. Note that if $\pi'$ is a result of the deviation of Player 1 following some path $\pi_{j'}$ for some index $j'$, we consider the $j$ which is also greater than $j'$.

Consider the strategy $\sigma_0^j$ of Player 0 which follows the play $\pi_j$. We know that $\underline{\mathbf{MP}}_0(\pi_j) > c_j$ and $ \underline{\mathbf{MP}}_1(\pi_j) = d_j$. We also know from \textbf{\cref{LemFinMemWitnessASV}}, the play $\pi_j$ does not cross a $(c_j, d_j)^{\epsilon}$-bad vertex. 

From \textbf{\cref{PropContradictionPlayEnforcable}}, we have that $\pi'$ can be enforced in response to strategy $\sigma_0^j$ of Player 0. Thus, we can establish that the play $\pi_j$ crosses a $(c_j, d_j)^{\epsilon}$-bad vertex $v$ as $\underline{\mathbf{MP}}_0(\pi') < c_j$ and $ \underline{\mathbf{MP}}_1(\pi') > d_j-\epsilon$ and $\pi'(0) = \pi_j(0) = v$. This implies that the existence of $\pi'$ is a contradiction. We conclude that the $\mathbf{ASV}^{\epsilon}(v)$ is achievable.

*********************************************

\sgcomment{How about the following? This does not use \cref{RemarkStratEqual} (same as the last bullet above) and \cref{PropContradictionPlayEnforcable}}.
Thus for every vertex $v$ in $\pi_*$, Player $1$ does not have a strategy such that $v \models \ll 1 \gg \underline{\mathbf{MP}}_0(\pi) \leqslant c_j \land \underline{\mathbf{MP}}_1(\pi) > d_j-\epsilon$.

Since $c' < c_j$ and $d' > d_j-\epsilon$, it also follows that Player $1$ does not have a strategy such that $v \models \ll 1 \gg \underline{\mathbf{MP}}_0(\pi) \leqslant c' \land \underline{\mathbf{MP}}_1(\pi) > d'$.
Stated otherwise, from the determinacy of multi-player mean-payoff games, we have that Player $0$ has a strategy to ensure $v \not \models \ll 1 \gg \underline{\mathbf{MP}}_0(\pi) \leqslant c' \land \underline{\mathbf{MP}}_1(\pi) > d'$ for every vertex $v$ appearing in $\pi_*$.
Using Proposition \ref{PropConvergenceStrategies}, since the path $\pi_*$ and the paths $\pi_i$, for all $i \in \mathbb{N}$ use the same set of vertices, Player $0$ can ensure $v \not \models \ll 1 \gg \underline{\mathbf{MP}}_0(\pi) \leqslant c' \land \underline{\mathbf{MP}}_1(\pi) > d'$ by choosing some strategy $\sigma_{j'}$ for $j' \ge j$.
Since we have that $\mathbf{ASV}(v) = \sup\limits_{\sigma_0 \in \Sigma_0} \mathbf{ASV}(\sigma_0)(v)$, and w.l.o.g. the sequence $\mathbf{ASV}(\sigma_i)(v)$ is non-decreasing for increasing $(i)_{i \in \mathbb{N}}$, it follows that the existence of $\pi'$ is a contradiction.

Now, we consider the case where the sequence ($d_i$)$_{i \in \mathbb{N}}$ is monotonically decreasing.
Again, assume for contradiction that $\mathbf{ASV}^{\epsilon}(v)$ is not achievable.  i.e. Player 1 deviates from $\sigma_0^*$ to enforce the path $\pi'$.
Since the sequence ($d_i$)$_{i \in \mathbb{N}}$ is monotonically decreasing, we know that there must exist $j \in \mathbb{N}$ such that 
\begin{inparaenum}[(i)]
\item $d' > d_j - \epsilon$, and $\forall i \geqslant j$, we have that $d' > d_i - \epsilon$, and
\item $c_j > c'$, which follows since ($c_i$)$_{i \in \mathbb{N}}$ is a strictly increasing sequence.
\end{inparaenum}
Thus for every vertex $v$ in $\pi_j$ in $\pi_*$, Player $1$ does not have a strategy such that $v \models \ll 1 \gg \underline{\mathbf{MP}}_0(\pi) \leqslant c_j \land \underline{\mathbf{MP}}_1(\pi) > d_j-\epsilon$.

Recall that, by \cref{PropConvergenceStrategies}, the set of vertices appearing in $\pi_*$ is the same as the set of vertices appearing in $\pi_j$.
Since $c' < c_j$ and $d' > d_j-\epsilon$, it also follows that $v \not \models \ll 1 \gg \underline{\mathbf{MP}}_0(\pi) \leqslant c' \land \underline{\mathbf{MP}}_1(\pi) > d'$.
Now the contradiction follows exactly as above where the sequence ($d_i$)$_{i \in \mathbb{N}}$ is monotonically non-decreasing.
\end{proof}